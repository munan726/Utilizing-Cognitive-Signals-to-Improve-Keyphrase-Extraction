{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weight = 'bert-base-uncased'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "max_len = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_path = r'D:\\LeStoreDownload\\AKE-copy01\\datas\\Election-Trec\\train.json'\n",
    "# train_path = '../datas/daily life/train.json'\n",
    "test_path = r'D:\\LeStoreDownload\\AKE-copy01\\datas\\Election-Trec\\test.json'\n",
    "# test_path = '../datas/daily life/test.json'\n",
    "\n",
    "train_file = json.load(open(train_path,'r',encoding='utf-8'))\n",
    "test_file = json.load(open(test_path, 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Append all words, eye-tracking signals, EEG signals and tags from training json to list\n",
    "train_sens, train_tags = [],[]\n",
    "train_Feature = []\n",
    "train_word_nums = []\n",
    "\n",
    "sens = ''\n",
    "nums = 0\n",
    "for key in train_file.keys():\n",
    "    tags = []\n",
    "    features = []\n",
    "    items = train_file[key]\n",
    "    sens = ''\n",
    "    nums = 0\n",
    "    for item in items:\n",
    "        sens += item[0]\n",
    "        sens += ' '\n",
    "        features.append(item[1:-1])               # ET+EEG: [1: -1]\n",
    "        tags.append(item[-1])\n",
    "        nums += 1\n",
    "    train_sens.append(sens.strip())\n",
    "    train_word_nums.append(nums)\n",
    "    train_Feature.append(features)\n",
    "    train_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Append all words, eye-tracking signals, EEG signals and tags from testing json to list\n",
    "test_sens, test_tags = [],[]\n",
    "test_Feature = []\n",
    "test_word_nums = []\n",
    "\n",
    "sens = ''\n",
    "nums = 0\n",
    "for key in test_file.keys():\n",
    "    tags = []\n",
    "    features = []\n",
    "    items = test_file[key]\n",
    "    sens = ''\n",
    "    nums = 0\n",
    "    for item in items:\n",
    "        sens += item[0]\n",
    "        sens += ' '\n",
    "        features.append(item[1:-1])                # ET+EEG: [1: -1]\n",
    "        tags.append(item[-1])\n",
    "        nums += 1\n",
    "    test_sens.append(sens.strip())\n",
    "    test_word_nums.append(nums)\n",
    "    test_Feature.append(features)\n",
    "    test_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(test_sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_to_ids = {'none': 0, 'B': 1, 'I': 2, 'E': 3, 'S': 4, \"O\": 5}\n",
    "# label_to_ids = {'O': 0, 'B': 1, 'I': 2, 'E': 3, 'S': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, old_features, tags):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.old_features = old_features\n",
    "        \n",
    "        self.labels = []\n",
    "        self.tokens = []\n",
    "        self.features = []\n",
    "        \n",
    "        self.input_ids = None\n",
    "        self.attention_masks = None\n",
    "\n",
    "    def encode(self):\n",
    "        for i in tqdm(range(len(self.texts))):\n",
    "          text = self.texts[i]\n",
    "          tag = self.tags[i]\n",
    "          feature = self.old_features[i]\n",
    "          tags, tokens, features = align_label(text, tag, feature)\n",
    "          self.labels.append(tags)\n",
    "          self.tokens.append(tokens)\n",
    "          self.features.append(features)\n",
    "          \n",
    "        self.features = np.array(self.features,float)\n",
    "        self.inputs = tokenizer(self.texts, max_length=max_len, add_special_tokens=True, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        self.input_ids = self.inputs['input_ids']\n",
    "        self.attention_masks = self.inputs['attention_mask']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx,:], self.attention_masks[idx,:], self.tokens[idx], torch.tensor(self.features[idx],dtype=torch.float32), torch.tensor(self.labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "\n",
    "\n",
    "def align_label(text, labels, features):\n",
    "    input = tokenizer(text, max_length=max_len, add_special_tokens=True, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    word_ids = input.word_ids()\n",
    "    input_ids = input['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    previous_word_idx = None\n",
    "    new_labels, new_features = [], []\n",
    "    no_features = [0 for _ in range(1, 26)]\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            new_labels.append('none')\n",
    "            new_features.append(no_features)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                new_labels.append(labels[word_idx])\n",
    "                new_features.append(features[word_idx])\n",
    "            except:\n",
    "                new_labels.append('none')\n",
    "                new_features.append(no_features)\n",
    "        else:\n",
    "            try:\n",
    "                new_labels.append(labels[word_idx] if label_all_tokens else 'none')\n",
    "                new_features.append(features[word_idx] if label_all_tokens else no_features)\n",
    "            except:\n",
    "                new_labels.append('none')\n",
    "                new_features.append(no_features)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    label_ids = [label_to_ids[label] for label in new_labels]\n",
    "    return label_ids, tokens, new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_sens, train_Feature, train_tags)\n",
    "train_dataset.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(test_sens, test_Feature, test_tags)\n",
    "test_dataset.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### construct bert  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BertNerModel(nn.Module):\n",
    "    def __init__(self,num_labels):\n",
    "        super(BertNerModel,self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(weight)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768+25,num_labels)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask,extra_features,token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        pooled_output = outputs[0]\n",
    "        bert_outputs = self.dropout(pooled_output)\n",
    "        \n",
    "        outputs = torch.concat((bert_outputs,extra_features[:,:,:]),-1)\n",
    "        # outputs = bert_outputs\n",
    "        outputs = self.classifier(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def TagConvert(raw_tags, words_set, poss=None):\n",
    "    true_tags = []\n",
    "    for i in range(raw_tags.shape[0]):\n",
    "      kw_list = []\n",
    "      nkw_list = \"\"\n",
    "      for j in range(len(raw_tags[i])):\n",
    "          item = raw_tags[i][j]\n",
    "          if item == 0:\n",
    "              continue\n",
    "          if poss !=None and j in poss[i]:\n",
    "              continue\n",
    "          # if item == 5:\n",
    "          #     continue\n",
    "          if item == 4:\n",
    "              kw_list.append(str(words_set[j][i]))\n",
    "          if item == 1:\n",
    "              nkw_list += str(words_set[j][i])\n",
    "          if item == 2:\n",
    "              nkw_list += \" \"\n",
    "              nkw_list += str(words_set[j][i])\n",
    "          if item == 3:\n",
    "              nkw_list += \" \"\n",
    "              nkw_list += str(words_set[j][i])\n",
    "              kw_list.append(nkw_list)\n",
    "              nkw_list = \"\"\n",
    "\n",
    "      true_tags.append(kw_list)\n",
    "    return true_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(predict_data, target_data, topk=3):\n",
    "  TRUE_COUNT, PRED_COUNT, GOLD_COUNT = 0.0, 0.0, 0.0\n",
    "  for index, words in enumerate(predict_data):\n",
    "      y_pred, y_true = None, target_data[index]\n",
    "\n",
    "      if type(predict_data) == str:\n",
    "          words = sorted(words.items(), key=lambda item: (-item[1], item[0]))\n",
    "          y_pred = [i[0] for i in words]\n",
    "      elif type(predict_data) == list:\n",
    "          y_pred = words\n",
    "\n",
    "      y_pred = y_pred[0: topk]\n",
    "      TRUE_NUM = len(set(y_pred) & set(y_true))\n",
    "      TRUE_COUNT += TRUE_NUM\n",
    "      PRED_COUNT += len(y_pred)\n",
    "      GOLD_COUNT += len(y_true)\n",
    "  # compute P\n",
    "  if PRED_COUNT != 0:\n",
    "      p = (TRUE_COUNT / PRED_COUNT)\n",
    "  else:\n",
    "      p = 0\n",
    "  # compute R\n",
    "  if GOLD_COUNT != 0:\n",
    "      r = (TRUE_COUNT / GOLD_COUNT)\n",
    "  else:\n",
    "      r = 0\n",
    "  # compute F1\n",
    "  if (r + p) != 0:\n",
    "      f1 = ((2 * r * p) / (r + p))\n",
    "  else:\n",
    "      f1 = 0\n",
    "\n",
    "  p = round(p * 100, 2)\n",
    "  r = round(r * 100, 2)\n",
    "  f1 = round(f1 * 100, 2)\n",
    "\n",
    "  return p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_f1(y_pred, y_true):\n",
    "    # flatten and convert to numpy array\n",
    "    y_true = y_true.view(-1)\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "    mask = np.where(y_true != 0)\n",
    "\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "model = BertNerModel(num_labels=6)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(),lr=5e-5,weight_decay=1e-2)\n",
    "loss_fn = CrossEntropyLoss(reduction='none', ignore_index=0)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "epochs = 5\n",
    "best_f1 = 0.0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_value = 0.0\n",
    "    model.train()\n",
    "    label_true, label_pred = [], []\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        optim.zero_grad()\n",
    "        input_ids, attention_masks, _, features, tags = batch\n",
    "        pred_tags = model(input_ids.to(device), attention_masks.to(device), features.to(device))\n",
    "\n",
    "        loss = loss_fn(pred_tags.permute(0,2,1),tags.to(device))\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        pred_tags = F.softmax(pred_tags,dim=-1)\n",
    "        pred_tags = torch.argmax(pred_tags,dim=-1)\n",
    "\n",
    "        y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "        label_true.extend(y_true)\n",
    "        label_pred.extend(y_pred)\n",
    "    \n",
    "        loss_value += loss.item()\n",
    "\n",
    "    label_train_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "\n",
    "    model.eval()\n",
    "    kw_true, kw_pred = [], []\n",
    "    label_true, label_pred = [],[]\n",
    "    for i,batch in enumerate(test_dataloader):\n",
    "      input_ids, attention_masks, tokens, features, tags = batch\n",
    "      with torch.no_grad():\n",
    "          for module in model.modules():\n",
    "              if isinstance(module, nn.Dropout):\n",
    "                  module.p = 0\n",
    "                  module.train(False)\n",
    "          pred_tags = model(input_ids.to(device), attention_masks.to(device), features.to(device))\n",
    "          pred_tags = F.softmax(pred_tags,dim=-1)\n",
    "          pred_tags = torch.argmax(pred_tags,dim=-1)\n",
    "\n",
    "      y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "      label_true.extend(y_true)\n",
    "      label_pred.extend(y_pred)\n",
    "\n",
    "      # more balance evaluate\n",
    "      poss = []\n",
    "      for i in range(len(tags)):\n",
    "          pos = []\n",
    "          for j in range(len(tags[i])):\n",
    "              if tags[i][j] == 0:\n",
    "                  pos.append(j)\n",
    "          poss.append(pos)\n",
    "           \n",
    "      kw_true.extend(TagConvert(tags,tokens))\n",
    "      kw_pred.extend(TagConvert(pred_tags,tokens,poss))\n",
    "\n",
    "    label_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "    P, R, F1 = evaluate(kw_true, kw_pred)\n",
    "    \n",
    "    if F1 > best_f1:\n",
    "        best_f1 = F1\n",
    "        torch.save(model.state_dict(),'./pretrain_pt/bert.pt')\n",
    "        \n",
    "    print(\"epoch{}:  loss:{:.2f}   train_f1_value:{:.2f}  test_f1_value:{:.2f}  kw_f1_value:{:.2f}\".format(\n",
    "        epoch+1, loss_value / len(train_dataloader), label_train_f1, label_f1, F1\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = BertNerModel(num_labels=6)\n",
    "model.load_state_dict(torch.load('./pretrain_pt/bert.pt'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model.eval()\n",
    "kw_true, kw_pred = [], []\n",
    "label_true, label_pred = [],[]\n",
    "for i,batch in enumerate(test_dataloader):\n",
    "    input_ids, attention_masks, tokens, extra_features, tags = batch\n",
    "    with torch.no_grad():\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = 0\n",
    "                module.train(False)\n",
    "        #pred_tags = model(input_ids.to(device), attention_masks.to(device))\n",
    "        pred_tags = model(input_ids.to(device), attention_masks.to(device), extra_features.to(device))\n",
    "        pred_tags = F.softmax(pred_tags,dim=-1)\n",
    "        pred_tags = torch.argmax(pred_tags,dim=-1)\n",
    "\n",
    "    y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "    label_true.extend(y_true)\n",
    "    label_pred.extend(y_pred)\n",
    "\n",
    "    # more balance evaluate\n",
    "    poss = []\n",
    "    for i in range(len(tags)):\n",
    "        pos = []\n",
    "        for j in range(len(tags[i])):\n",
    "            if tags[i][j] == 0:\n",
    "                pos.append(j)\n",
    "        poss.append(pos)\n",
    "        \n",
    "    kw_true.extend(TagConvert(tags,tokens))\n",
    "    kw_pred.extend(TagConvert(pred_tags,tokens,poss))\n",
    "\n",
    "label_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "P, R, F1 = evaluate(kw_true, kw_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(P)\n",
    "print(R)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fs_num = 25  # 定义额外特征的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# model = BertNerModelWithSoftAttention(num_labels=6)\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "model = BertNerModelWithAttention(num_labels=6)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "#optim = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
    "optim = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)  # 尝试降低学习率\n",
    "\n",
    "loss_fn = CrossEntropyLoss(reduction='none', ignore_index=0)\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "epochs = 5\n",
    "best_f1 = 0.0\n",
    "num_labels = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs = 5\n",
    "best_f1 = 0.0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_value = 0.0\n",
    "    model.train()\n",
    "    label_true, label_pred = [], []\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optim.zero_grad()\n",
    "        input_ids, attention_masks, _, features, tags = batch\n",
    "        pred_tags, _ = model(input_ids.to(device), attention_masks.to(device), features.to(device))\n",
    "\n",
    "        loss = loss_fn(pred_tags.permute(0, 2, 1), tags.to(device))\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        pred_tags = F.softmax(pred_tags, dim=-1)\n",
    "        pred_tags = torch.argmax(pred_tags, dim=-1)\n",
    "\n",
    "        y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "        label_true.extend(y_true)\n",
    "        label_pred.extend(y_pred)\n",
    "\n",
    "        loss_value += loss.item()\n",
    "\n",
    "    label_train_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "\n",
    "    model.eval()\n",
    "    kw_true, kw_pred = [], []\n",
    "    label_true, label_pred = [], []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        input_ids, attention_masks, tokens, features, tags = batch\n",
    "        with torch.no_grad():\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, nn.Dropout):\n",
    "                    module.p = 0\n",
    "                    module.train(False)\n",
    "            pred_tags, _ = model(input_ids.to(device), attention_masks.to(device), features.to(device))\n",
    "            pred_tags = F.softmax(pred_tags, dim=-1)\n",
    "            pred_tags = torch.argmax(pred_tags, dim=-1)\n",
    "\n",
    "        y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "        label_true.extend(y_true)\n",
    "        label_pred.extend(y_pred)\n",
    "\n",
    "        poss = []\n",
    "        for i in range(len(tags)):\n",
    "            pos = []\n",
    "            for j in range(len(tags[i])):\n",
    "                if tags[i][j] == 0:\n",
    "                    pos.append(j)\n",
    "            poss.append(pos)\n",
    "\n",
    "        kw_true.extend(TagConvert(tags, tokens))\n",
    "        kw_pred.extend(TagConvert(pred_tags, tokens, poss))\n",
    "\n",
    "    label_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "    P, R, F1 = evaluate(kw_true, kw_pred)\n",
    "\n",
    "    if F1 > best_f1:\n",
    "        best_f1 = F1\n",
    "        torch.save(model.state_dict(), './pretrain_pt/bert_with_attention.pt')\n",
    "\n",
    "    print(\"epoch{}:  loss:{:.2f}   train_f1_value:{:.2f}  test_f1_value:{:.2f}  kw_f1_value:{:.2f}\".format(\n",
    "        epoch + 1, loss_value / len(train_dataloader), label_train_f1, label_f1, F1\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = BertNerModelWithAttention(num_labels=6)\n",
    "model.load_state_dict(torch.load('./pretrain_pt/bert_with_attention.pt'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# 加载最佳模型权重\n",
    "model.load_state_dict(torch.load('./pretrain_pt/bert_with_attention.pt'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def inference_and_evaluate(test_dataloader, model, device):\n",
    "    kw_true, kw_pred = [], []\n",
    "    label_true, label_pred = [], []\n",
    "\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        input_ids, attention_masks, tokens, features, tags = batch\n",
    "        with torch.no_grad():\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, nn.Dropout):\n",
    "                    module.p = 0\n",
    "                    module.train(False)\n",
    "            outputs = model(input_ids.to(device), attention_masks.to(device), features.to(device))\n",
    "            pred_tags = outputs[0] if isinstance(outputs, tuple) else outputs  # Handle tuple output\n",
    "            pred_tags = F.softmax(pred_tags, dim=-1)\n",
    "            pred_tags = torch.argmax(pred_tags, dim=-1)\n",
    "\n",
    "        y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "        label_true.extend(y_true)\n",
    "        label_pred.extend(y_pred)\n",
    "\n",
    "        # more balance evaluate\n",
    "        poss = []\n",
    "        for i in range(len(tags)):\n",
    "            pos = []\n",
    "            for j in range(len(tags[i])):\n",
    "                if tags[i][j] == 0:\n",
    "                    pos.append(j)\n",
    "            poss.append(pos)\n",
    "\n",
    "        kw_true.extend(TagConvert(tags, tokens))\n",
    "        kw_pred.extend(TagConvert(pred_tags, tokens, poss))\n",
    "\n",
    "    label_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "    P, R, F1 = evaluate(kw_true, kw_pred)\n",
    "\n",
    "    return label_f1, P, R, F1\n",
    "\n",
    "# 调用推理和评价函数\n",
    "label_f1, P, R, F1 = inference_and_evaluate(test_dataloader, model, device)\n",
    "\n",
    "print(f\"Label F1 Score: {label_f1:.2f}\")\n",
    "print(f\"Precision: {P:.2f}\")\n",
    "print(f\"Recall: {R:.2f}\")\n",
    "print(f\"F1 Score: {F1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ###定义词级别和句子级别的注意力层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "层注意力 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, regularizer=None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.context = nn.Parameter(torch.FloatTensor(hidden_dim, 1) * 0.01)\n",
    "        nn.init.normal_(self.context, mean=0.0, std=0.05)\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attention_in = torch.exp(torch.squeeze(torch.matmul(x, self.context), -1))\n",
    "        if mask is not None:\n",
    "            attention_in = attention_in * mask.float()\n",
    "        attention = attention_in / torch.unsqueeze(torch.sum(attention_in, -1), -1)\n",
    "        weighted_sum = torch.bmm(attention.unsqueeze(1), x).squeeze(1)\n",
    "        return weighted_sum\n",
    "\n",
    "class BertHANModel(nn.Module):\n",
    "    def __init__(self, num_labels, hidden_dim=768):\n",
    "        super(BertHANModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.word_attention = Attention(hidden_dim)\n",
    "        self.sentence_attention = Attention(hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sentence_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]  # shape: (batch_size, seq_length, hidden_dim)\n",
    "        mask = (sentence_ids != 0).float()\n",
    "        word_attention_output = self.word_attention(sequence_output, mask)\n",
    "        sentence_attention_output = self.sentence_attention(word_attention_output, mask)\n",
    "        logits = self.classifier(sentence_attention_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertHANModel(num_labels=6)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none', ignore_index=0)\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.context = nn.Parameter(torch.FloatTensor(hidden_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.context)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attention_in = torch.tanh(torch.matmul(x, self.context))\n",
    "        attention_in = torch.squeeze(attention_in, -1)\n",
    "        if mask is not None:\n",
    "            attention_in = attention_in * mask.float()\n",
    "        attention_weights = F.softmax(attention_in, dim=-1)\n",
    "        weighted_sum = torch.bmm(attention_weights.unsqueeze(1), x).squeeze(1)\n",
    "        return weighted_sum\n",
    "\n",
    "class BertHANModel(nn.Module):\n",
    "    def __init__(self, num_labels, hidden_dim=768, rnn_dim=256):\n",
    "        super(BertHANModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.word_attention = Attention(hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim, rnn_dim, batch_first=True, bidirectional=True)\n",
    "        self.sentence_attention = Attention(rnn_dim * 2)\n",
    "        self.classifier = nn.Linear(rnn_dim * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_outputs[0]  # shape: (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "        # Word-level attention\n",
    "        word_attention_output = self.word_attention(sequence_output)\n",
    "\n",
    "        # Sentence-level GRU\n",
    "        rnn_output, _ = self.rnn(word_attention_output.unsqueeze(1))\n",
    "\n",
    "        # Sentence-level attention\n",
    "        sentence_attention_output = self.sentence_attention(rnn_output)\n",
    "\n",
    "        logits = self.classifier(sentence_attention_output).unsqueeze(1)  # shape: (batch_size, 1, num_labels)\n",
    "        return logits.expand(-1, sequence_output.size(1), -1)  # shape: (batch_size, seq_length, num_labels)\n",
    "\n",
    "def calculate_f1(y_pred, y_true):\n",
    "    y_true = y_true.view(-1)\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    mask = np.where(y_true != 0)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    return y_pred, y_true\n",
    "\n",
    "def TagConvert(raw_tags, words_set, poss=None):\n",
    "    true_tags = []\n",
    "    for i in range(raw_tags.shape[0]):\n",
    "        kw_list = []\n",
    "        nkw_list = \"\"\n",
    "        for j in range(len(raw_tags[i])):\n",
    "            item = raw_tags[i][j]\n",
    "            if item == 0:\n",
    "                continue\n",
    "            if poss != None and j in poss[i]:\n",
    "                continue\n",
    "            if item == 4:\n",
    "                kw_list.append(str(words_set[j][i]))\n",
    "            if item == 1:\n",
    "                nkw_list += str(words_set[j][i])\n",
    "            if item == 2:\n",
    "                nkw_list += \" \"\n",
    "                nkw_list += str(words_set[j][i])\n",
    "            if item == 3:\n",
    "                nkw_list += \" \"\n",
    "                nkw_list += str(words_set[j][i])\n",
    "                kw_list.append(nkw_list)\n",
    "                nkw_list = \"\"\n",
    "        true_tags.append(kw_list)\n",
    "    return true_tags\n",
    "\n",
    "def evaluate(predict_data, target_data, topk=3):\n",
    "    TRUE_COUNT, PRED_COUNT, GOLD_COUNT = 0.0, 0.0, 0.0\n",
    "    for index, words in enumerate(predict_data):\n",
    "        y_pred, y_true = None, target_data[index]\n",
    "        if type(predict_data) == str:\n",
    "            words = sorted(words.items(), key=lambda item: (-item[1], item[0]))\n",
    "            y_pred = [i[0] for i in words]\n",
    "        elif type(predict_data) == list:\n",
    "            y_pred = words\n",
    "        y_pred = y_pred[0: topk]\n",
    "        TRUE_NUM = len(set(y_pred) & set(y_true))\n",
    "        TRUE_COUNT += TRUE_NUM\n",
    "        PRED_COUNT += len(y_pred)\n",
    "        GOLD_COUNT += len(y_true)\n",
    "    if PRED_COUNT != 0:\n",
    "        p = (TRUE_COUNT / PRED_COUNT)\n",
    "    else:\n",
    "        p = 0\n",
    "    if GOLD_COUNT != 0:\n",
    "        r = (TRUE_COUNT / GOLD_COUNT)\n",
    "    else:\n",
    "        r = 0\n",
    "    if (r + p) != 0:\n",
    "        f1 = ((2 * r * p) / (r + p))\n",
    "    else:\n",
    "        f1 = 0\n",
    "    p = round(p * 100, 2)\n",
    "    r = round(r * 100, 2)\n",
    "    f1 = round(f1 * 100, 2)\n",
    "    return p, r, f1\n",
    "\n",
    "# 假设已经定义了数据集和数据加载器\n",
    "# train_dataloader = ...\n",
    "# test_dataloader = ...\n",
    "\n",
    "# 训练和评估\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertHANModel(num_labels=6)\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none', ignore_index=0)\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "epochs = 5\n",
    "best_f1 = 0.0\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_value = 0.0\n",
    "    model.train()\n",
    "    label_true, label_pred = [], []\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        tags = batch[4].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            pred_tags = model(input_ids, attention_masks)\n",
    "\n",
    "            # 展平 pred_tags 和 tags 以匹配形状\n",
    "            pred_tags = pred_tags.reshape(-1, pred_tags.size(-1))\n",
    "            tags = tags.reshape(-1)\n",
    "\n",
    "            #print(f\"pred_tags shape: {pred_tags.shape}, tags shape: {tags.shape}\")\n",
    "\n",
    "            loss = loss_fn(pred_tags, tags)\n",
    "            loss = loss.mean()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "\n",
    "        pred_tags = F.softmax(pred_tags, dim=-1)\n",
    "        pred_tags = torch.argmax(pred_tags, dim=-1)\n",
    "        y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "        label_true.extend(y_true)\n",
    "        label_pred.extend(y_pred)\n",
    "        loss_value += loss.item()\n",
    "\n",
    "    label_train_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "\n",
    "    model.eval()\n",
    "    kw_true, kw_pred = [], []\n",
    "    label_true, label_pred = [], []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        tokens = batch[2]  # tokens 不是 Tensor，直接使用\n",
    "        tags = batch[4].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, nn.Dropout):\n",
    "                    module.p = 0\n",
    "                    module.train(False)\n",
    "            with autocast():\n",
    "                pred_tags = model(input_ids, attention_masks)\n",
    "                pred_tags = F.softmax(pred_tags, dim=-1)\n",
    "                pred_tags = torch.argmax(pred_tags, dim=-1)\n",
    "\n",
    "        y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "        label_true.extend(y_true)\n",
    "        label_pred.extend(y_pred)\n",
    "\n",
    "        poss = []\n",
    "        for i in range(len(tags)):\n",
    "            pos = []\n",
    "            for j in range(len(tags[i])):\n",
    "                if tags[i][j] == 0:\n",
    "                    pos.append(j)\n",
    "            poss.append(pos)\n",
    "\n",
    "        kw_true.extend(TagConvert(tags, tokens))\n",
    "        kw_pred.extend(TagConvert(pred_tags, tokens, poss))\n",
    "\n",
    "    label_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "    P, R, F1 = evaluate(kw_true, kw_pred)\n",
    "\n",
    "    if F1 > best_f1:\n",
    "        best_f1 = F1\n",
    "        torch.save(model.state_dict(), './pretrain_pt/bert_HAtten.pt')\n",
    "\n",
    "    print(\"epoch{}:  loss:{:.2f}   train_f1_value:{:.2f}  test_f1_value:{:.2f}  kw_f1_value:{:.2f}\".format(\n",
    "        epoch + 1, loss_value / len(train_dataloader), label_train_f1, label_f1, F1\n",
    "    ))\n",
    "\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, num_labels):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BertHANModel(num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def calculate_f1(y_pred, y_true):\n",
    "    y_true = y_true.view(-1)\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    mask = np.where(y_true != 0)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    return y_pred, y_true\n",
    "\n",
    "def TagConvert(raw_tags, words_set, poss=None):\n",
    "    true_tags = []\n",
    "    for i in range(raw_tags.shape[0]):\n",
    "        kw_list = []\n",
    "        nkw_list = \"\"\n",
    "        for j in range(len(raw_tags[i])):\n",
    "            item = raw_tags[i][j]\n",
    "            if item == 0:\n",
    "                continue\n",
    "            if poss != None and j in poss[i]:\n",
    "                continue\n",
    "            if item == 4:\n",
    "                kw_list.append(str(words_set[j][i]))\n",
    "            if item == 1:\n",
    "                nkw_list += str(words_set[j][i])\n",
    "            if item == 2:\n",
    "                nkw_list += \" \"\n",
    "                nkw_list += str(words_set[j][i])\n",
    "            if item == 3:\n",
    "                nkw_list += \" \"\n",
    "                nkw_list += str(words_set[j][i])\n",
    "                kw_list.append(nkw_list)\n",
    "                nkw_list = \"\"\n",
    "        true_tags.append(kw_list)\n",
    "    return true_tags\n",
    "\n",
    "def evaluate(predict_data, target_data, topk=3):\n",
    "    TRUE_COUNT, PRED_COUNT, GOLD_COUNT = 0.0, 0.0, 0.0\n",
    "    for index, words in enumerate(predict_data):\n",
    "        y_pred, y_true = None, target_data[index]\n",
    "        if type(predict_data) == str:\n",
    "            words = sorted(words.items(), key=lambda item: (-item[1], item[0]))\n",
    "            y_pred = [i[0] for i in words]\n",
    "        elif type(predict_data) == list:\n",
    "            y_pred = words\n",
    "        y_pred = y_pred[0: topk]\n",
    "        TRUE_NUM = len(set(y_pred) & set(y_true))\n",
    "        TRUE_COUNT += TRUE_NUM\n",
    "        PRED_COUNT += len(y_pred)\n",
    "        GOLD_COUNT += len(y_true)\n",
    "    if PRED_COUNT != 0:\n",
    "        p = (TRUE_COUNT / PRED_COUNT)\n",
    "    else:\n",
    "        p = 0\n",
    "    if GOLD_COUNT != 0:\n",
    "        r = (TRUE_COUNT / GOLD_COUNT)\n",
    "    else:\n",
    "        r = 0\n",
    "    if (r + p) != 0:\n",
    "        f1 = ((2 * r * p) / (r + p))\n",
    "    else:\n",
    "        f1 = 0\n",
    "    p = round(p * 100, 2)\n",
    "    r = round(r * 100, 2)\n",
    "    f1 = round(f1 * 100, 2)\n",
    "    return p, r, f1\n",
    "\n",
    "def predict_and_evaluate(model, dataloader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    label_true, label_pred = [], []\n",
    "    kw_true, kw_pred = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_masks = batch[1].to(device)\n",
    "            tokens = batch[2]\n",
    "            tags = batch[4].to(device)\n",
    "\n",
    "            pred_tags = model(input_ids, attention_masks)\n",
    "            pred_tags = torch.argmax(pred_tags, dim=-1)\n",
    "\n",
    "            y_pred, y_true = calculate_f1(pred_tags, tags)\n",
    "            label_true.extend(y_true)\n",
    "            label_pred.extend(y_pred)\n",
    "\n",
    "            poss = []\n",
    "            for i in range(len(tags)):\n",
    "                pos = []\n",
    "                for j in range(len(tags[i])):\n",
    "                    if tags[i][j] == 0:\n",
    "                        pos.append(j)\n",
    "                poss.append(pos)\n",
    "            kw_true.extend(TagConvert(tags, tokens))\n",
    "            kw_pred.extend(TagConvert(pred_tags, tokens, poss))\n",
    "\n",
    "    label_f1 = f1_score(label_true, label_pred, average='macro')\n",
    "    P, R, F1 = evaluate(kw_true, kw_pred)\n",
    "    return label_f1, P, R, F1\n",
    "\n",
    "# 加载模型\n",
    "model_path = './pretrain_pt/bert_HAtten.pt'\n",
    "num_labels = 6\n",
    "model = load_model(model_path, num_labels)\n",
    "\n",
    "# 假设 test_dataloader 已经定义好\n",
    "label_f1, P, R, F1 = predict_and_evaluate(model, test_dataloader)\n",
    "\n",
    "print(f\"label_f1: {label_f1:.2f}, Precision: {P:.2f}, Recall: {R:.2f}, F1: {F1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
